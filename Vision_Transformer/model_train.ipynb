{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def is_colab():\n",
    "    try:\n",
    "        from google.colab import drive\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "if is_colab():\n",
    "    %cd /content/\n",
    "    !rm -rf pytorch_random\n",
    "    \n",
    "    !git clone https://github.com/Priyanshu-hawk/pytorch_random.git\n",
    "    if os.getcwd() != '/content/pytorch_random':\n",
    "        %cd /content/pytorch_random\n",
    "\n",
    "%cd Vision_Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "foodDataset = datasets.Food101(\"../dataset\", download=True)\n",
    "\n",
    "\n",
    "BASE_FOOD_DS = os.path.join(\"../dataset/food-101\")\n",
    "print(BASE_FOOD_DS)\n",
    "BASE_FOOD_IMGS = os.path.join(BASE_FOOD_DS, \"images\")\n",
    "BASE_FOOD_META = os.path.join(BASE_FOOD_DS, \"meta\")\n",
    "TRAIN_JSON_PATH = os.path.join(BASE_FOOD_META, \"train.json\")\n",
    "TEST_JSON_PATH = os.path.join(BASE_FOOD_META, \"test.json\")\n",
    "CLASSES_TEXT = os.path.join(BASE_FOOD_META, \"classes.txt\")\n",
    "\n",
    "TRAIN_JSON = json.load(open(TRAIN_JSON_PATH))\n",
    "TEST_JSON = json.load(open(TEST_JSON_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = []\n",
    "with open(CLASSES_TEXT, \"r\") as f:\n",
    "    for f_p in f.readlines():\n",
    "        CLASS_NAMES.append(f_p.strip(\"\\n\"))\n",
    "CLASS_NAMES[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_DS = \"../dataset/food101_torch\"\n",
    "\n",
    "# run once\n",
    "# creating dataset acording to pytorch format\n",
    "\n",
    "#format = train/class_name/imgs \n",
    "#BASE_FOOD_IMGS,TRAIN_JSON[CLASS_NAMES[rand_Class_name]][random.randint(0,750)]+\".jpg\"\n",
    "\n",
    "\n",
    "\n",
    "#train loop\n",
    "if not os.path.exists(NEW_DS):\n",
    "    os.makedirs(NEW_DS, exist_ok=True)\n",
    "    for c_i , _class in enumerate(tqdm(CLASS_NAMES)):\n",
    "        print(_class)\n",
    "        class_path = os.path.join(NEW_DS, \"train\", _class)\n",
    "        os.makedirs(class_path, exist_ok=True)\n",
    "        i = 0\n",
    "        for _img in range(0, len(TRAIN_JSON[CLASS_NAMES[0]])):\n",
    "            img_path = os.path.join(class_path, str(i)+\".jpg\")\n",
    "            # print(os.path.join(BASE_FOOD_IMGS,TRAIN_JSON[CLASS_NAMES[c_i][_img]]+\".jpg\"))\n",
    "            shutil.copy(os.path.join(BASE_FOOD_IMGS,TRAIN_JSON[CLASS_NAMES[c_i]][_img]+\".jpg\"),  img_path)\n",
    "            i+=1\n",
    "\n",
    "    #test loop\n",
    "    for c_i , _class in enumerate(tqdm(CLASS_NAMES)):\n",
    "        print(_class)\n",
    "        class_path = os.path.join(NEW_DS, \"test\", _class)\n",
    "        os.makedirs(class_path, exist_ok=True)\n",
    "        i = 0\n",
    "        for _img in range(0, len(TEST_JSON[CLASS_NAMES[0]])):\n",
    "            img_path = os.path.join(class_path, str(i)+\".jpg\")\n",
    "            # print(os.path.join(BASE_FOOD_IMGS,TRAIN_JSON[CLASS_NAMES[c_i][_img]]+\".jpg\"))\n",
    "            shutil.copy(os.path.join(BASE_FOOD_IMGS,TEST_JSON[CLASS_NAMES[c_i]][_img]+\".jpg\"),  img_path)\n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_path = \"../dataset/pizza_steak_sushi/data\"\n",
    "# image_path = \"../dataset/pizza_steak_sushi/\"\n",
    "\n",
    "train_dir = os.path.join(image_path, \"train\")\n",
    "test_dir = os.path.join(image_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from data_setup import createDataloader\n",
    "\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)) ,\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_DL, test_DL, _classes = createDataloader(train_dir, test_dir, 64, train_transform, train_transform )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_DL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total classes: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"total classes:\",len(_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = 224\n",
    "W = 224\n",
    "C = 3\n",
    "P = 16\n",
    "N = int((H * W) / P**2)\n",
    "N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "img = torch.randn(1, C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 197, 768])\n"
     ]
    }
   ],
   "source": [
    "# compliling eq.1 in a single block of code\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "single_img = next(iter(test_DL))[0][0].unsqueeze(0) # single img of batch 1\n",
    "\n",
    "# patch embb code\n",
    "class PatchEmbeddingWithPosEmbb(nn.Module):\n",
    "    def __init__(self, color_channel, out_num_patch, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv_layer = nn.Conv2d(in_channels=color_channel,\n",
    "                               out_channels=out_num_patch,\n",
    "                               kernel_size=patch_size,\n",
    "                               stride=patch_size,\n",
    "                               padding=0)\n",
    "        self.flat = nn.Flatten(start_dim=2, end_dim=3)\n",
    "\n",
    "    # Class Token (B, 1, embedding_token) concat to Patched image (B, no_of_patches, embedding_token) = (B, 1+no_of_patches, embedding_token)\n",
    "    def class_token(self, patch_embb):\n",
    "        class_token_layer = nn.Parameter(torch.randn(1, 1, patch_embb.shape[2]), requires_grad=True).to(device=device) # (1, 1, 768)\n",
    "        class_token_expanded = class_token_layer.expand(patch_embb.shape[0], -1, -1) # expending to the batch\n",
    "        return torch.cat((class_token_expanded, patch_embb),dim=1)\n",
    "\n",
    "    # Adding Patched Embb with Class Token - (B, 1+no_of_patches, Emmbed_dim)\n",
    "    def position_embbeding(self, patch_embb):\n",
    "        pos_embb_layer = nn.Parameter(torch.randn(patch_embb.shape), requires_grad=True).to(device)\n",
    "        return pos_embb_layer + patch_embb # (B, 196, 768) + (B, 196, 768)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x) # Patched Imgae - (batch, C, H, W) --> (Batch, no_of_patches, embedding_dim patches)\n",
    "        x = self.flat(x).permute(0,2,1)\n",
    "        x = self.class_token(x)\n",
    "        return self.position_embbeding(x)\n",
    "\n",
    "pe = PatchEmbeddingWithPosEmbb(3, 768, 16).to(device)\n",
    "gpu_img = img.to(device)\n",
    "input_patch_embedding = pe(gpu_img)\n",
    "print(input_patch_embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "PatchEmbeddingWithPosEmbb (PatchEmbeddingWithPosEmbb)   [1, 3, 224, 224]     [1, 197, 768]        --                   True\n",
       "├─Conv2d (conv_layer)                                   [1, 3, 224, 224]     [1, 768, 14, 14]     590,592              True\n",
       "├─Flatten (flat)                                        [1, 768, 14, 14]     [1, 768, 196]        --                   --\n",
       "=======================================================================================================================================\n",
       "Total params: 590,592\n",
       "Trainable params: 590,592\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 115.76\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 1.20\n",
       "Params size (MB): 2.36\n",
       "Estimated Total Size (MB): 4.17\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(pe, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, embb_dim: int, num_heads: int, attn_dropout: 0):\n",
    "        super().__init__()\n",
    "\n",
    "        # layernorm\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embb_dim)\n",
    "\n",
    "        # multi head, we have (B, NumPatch, EmbedDim) or (B, Seq, Feat)\n",
    "        self.multiHead = nn.MultiheadAttention(embed_dim=embb_dim, num_heads=num_heads, dropout=attn_dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        atten_out, _ = self.multiHead(query=x, key=x, value=x,need_weights=False)\n",
    "        return atten_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, preceptron: int,embb_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # layernorm\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embb_dim)\n",
    "\n",
    "        self.mlp_layer = nn.Sequential(\n",
    "            nn.Linear(in_features=embb_dim, out_features=preceptron),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(in_features=preceptron, out_features=embb_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        return self.mlp_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, color_c: int, out_patch_num: int, patch_size: int, heads: int, attn_dropout: float, perceptron: int, mlp_dropout: float ):\n",
    "        super().__init__()\n",
    "        self.MSA = MultiheadSelfAttention(embb_dim=out_patch_num, num_heads= heads, attn_dropout=attn_dropout)\n",
    "        self.MLP = MultiLayerPerceptron(perceptron, embb_dim=out_patch_num, dropout=mlp_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        msa_out = self.MSA(x) + x\n",
    "        mlp_out = self.MLP(msa_out) + msa_out\n",
    "        return mlp_out\n",
    "\n",
    "# transformer = TransformerEncoderBlock(3, 768, 16, 12, 0, 3072, 0).to(device)\n",
    "# transformer(input_patch_embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerEncoderLayer(\n",
       "  (self_attn): MultiheadAttention(\n",
       "    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  (activation): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768,\n",
    "                                                             nhead=12,\n",
    "                                                             dim_feedforward=3072,\n",
    "                                                             activation=nn.GELU(),\n",
    "                                                             device=device,\n",
    "                                                             batch_first=True,\n",
    "                                                             norm_first=True,\n",
    "                                                             dropout=0.1)\n",
    "\n",
    "torch_transformer_encoder_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "TransformerEncoderLayer (TransformerEncoderLayer)  [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "├─LayerNorm (norm1)                                [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "├─MultiheadAttention (self_attn)                   [32, 197, 768]       [32, 197, 768]       2,362,368            True\n",
       "├─Dropout (dropout1)                               [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "├─LayerNorm (norm2)                                [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "├─Linear (linear1)                                 [32, 197, 768]       [32, 197, 3072]      2,362,368            True\n",
       "├─GELU (activation)                                [32, 197, 3072]      [32, 197, 3072]      --                   --\n",
       "├─Dropout (dropout)                                [32, 197, 3072]      [32, 197, 3072]      --                   --\n",
       "├─Linear (linear2)                                 [32, 197, 3072]      [32, 197, 768]       2,360,064            True\n",
       "├─Dropout (dropout2)                               [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "==================================================================================================================================\n",
       "Total params: 7,087,872\n",
       "Trainable params: 7,087,872\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 151.22\n",
       "==================================================================================================================================\n",
       "Input size (MB): 19.37\n",
       "Forward/backward pass size (MB): 271.12\n",
       "Params size (MB): 18.90\n",
       "Estimated Total Size (MB): 309.39\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(torch_transformer_encoder_layer,\n",
    "        input_size=(32, 197, 768),\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VitArch(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size: int = 224,\n",
    "                 in_channel: int=3,\n",
    "                 batch_size: int = 32,\n",
    "                 patch_size: int = 16,\n",
    "                 num_transformer_layer:int = 12,\n",
    "                 embedding_dim: int = 768,\n",
    "                 mlp_size: int = 3072,\n",
    "                 num_heads: int=12,\n",
    "                 atten_dropout: float=0,\n",
    "                 mlp_dropout: float=0.1,\n",
    "                 embedding_dropout=0.1,\n",
    "                 num_classes=1000\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.patch_em = PatchEmbeddingWithPosEmbb(in_channel, embedding_dim, patch_size)\n",
    "        # nn.TransformerEncoderLayer(d_model=embedding_dim,\n",
    "        #                                                       nhead=num_heads,\n",
    "        #                                                       dim_feedforward=3072,\n",
    "        #                                                       dropout=atten_dropout)\n",
    "        self.embeding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(color_c=in_channel, out_patch_num=embedding_dim,\n",
    "                                                           patch_size=patch_size, heads=num_heads, attn_dropout=atten_dropout,\n",
    "                                                           perceptron=mlp_size, mlp_dropout=mlp_dropout) for _ in range(num_transformer_layer)])\n",
    "\n",
    "        self.classifer = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_em(x)\n",
    "        x = self.embeding_dropout(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        # print(x.shape, x[:, 0].shape)\n",
    "        return self.classifer(x[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VitArch(num_classes=len(_classes)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type (var_name))                                 Input Shape          Output Shape         Param #              Trainable\n",
       "=======================================================================================================================================\n",
       "VitArch (VitArch)                                       [32, 3, 224, 224]    [32, 101]            --                   True\n",
       "├─Dropout (embeding_dropout)                            [32, 3, 224, 224]    [32, 3, 224, 224]    --                   --\n",
       "├─PatchEmbeddingWithPosEmbb (patch_em)                  [32, 3, 224, 224]    [32, 197, 768]       --                   True\n",
       "│    └─Conv2d (conv_layer)                              [32, 3, 224, 224]    [32, 768, 14, 14]    590,592              True\n",
       "│    └─Flatten (flat)                                   [32, 768, 14, 14]    [32, 768, 196]       --                   --\n",
       "├─Dropout (embeding_dropout)                            [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "├─Sequential (transformer_encoder)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    └─TransformerEncoderBlock (0)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (1)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (2)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (3)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (4)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (5)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (6)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (7)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (8)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (9)                      [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (10)                     [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "│    └─TransformerEncoderBlock (11)                     [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─MultiheadSelfAttention (MSA)                [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
       "│    │    └─MultiLayerPerceptron (MLP)                  [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
       "├─Sequential (classifer)                                [32, 768]            [32, 101]            --                   True\n",
       "│    └─LayerNorm (0)                                    [32, 768]            [32, 768]            1,536                True\n",
       "│    └─Linear (1)                                       [32, 768]            [32, 101]            77,669               True\n",
       "=======================================================================================================================================\n",
       "Total params: 85,724,261\n",
       "Trainable params: 85,724,261\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 5.52\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3292.23\n",
       "Params size (MB): 229.50\n",
       "Estimated Total Size (MB): 3541.00\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(vit, input_size=(32, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], col_width=20, row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:02<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from utils import trainit\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optim = torch.optim.Adam(vit.parameters(), lr=3e-3, betas=(0.9, 0.999), weight_decay=0.3)\n",
    "\n",
    "trainit(vit, train_DL, test_DL, loss_fn, optim, 50, True, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Pre-Train weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# image_path = \"../dataset/pizza_steak_sushi/data\"\n",
    "image_path = \"../dataset/food101_torch\"\n",
    "\n",
    "train_dir = os.path.join(image_path, \"train\")\n",
    "test_dir = os.path.join(image_path, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, torch\n",
    "from torch import nn\n",
    "# 1. Get pretrained weights for ViT-Base\n",
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total classes: 101\n"
     ]
    }
   ],
   "source": [
    "from data_setup import createDataloader\n",
    "\n",
    "train_transform = pretrained_vit_weights.transforms()\n",
    "\n",
    "train_DL, test_DL, _classes = createDataloader(train_dir, test_dir, 32, train_transform, train_transform)\n",
    "\n",
    "print(\"total classes:\",len(_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
    "# set_seeds()\n",
    "pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(_classes)).to(device)\n",
    "# pretrained_vit # uncomment for model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 101]            768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n",
       "├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n",
       "├─Linear (heads)                                             [32, 768]            [32, 101]            77,669               True\n",
       "============================================================================================================================================\n",
       "Total params: 85,876,325\n",
       "Trainable params: 77,669\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (G): 5.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.76\n",
       "Params size (MB): 229.50\n",
       "Estimated Total Size (MB): 3579.53\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Print a summary using torchinfo (uncomment for actual output)\n",
    "summary(model=pretrained_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from utils import trainit\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "optim = torch.optim.Adam(pretrained_vit.parameters(), lr=1e-3)\n",
    "\n",
    "trainit(pretrained_vit, train_DL, test_DL, loss_fn, optim, 50, True, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
